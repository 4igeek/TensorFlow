{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Evaluation Methods\n",
    "\n",
    "In this notebook we're going to look closer at the most common classification evaluation methods available to use from TensorFlow. \n",
    "\n",
    "When compiling our models we usually define a \"metrics\" variable and pass it an evaluation method i.e. \"accuracy\".\n",
    "\n",
    "**Key:** tp = true positive, tn = true negative, fp = false positive, fn = false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Classification Evaluation Methods](../Reference/Images/ClassificationEvaluationMethods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Classification Evaluation Works\n",
    "When we call one of the methods shown above, the various functions will look at the results that it got from the network and it will look at the actual (true) labels and it will compare them to see how accurate our model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Aside from the various metrics that we can use to evaluate our model, we can also evaluate our model by creating a confusion matrix. A confusion matrix works well with problems that have few potential classes (labels). This can be achieved in TensorFlow using a custom method or by using [scikit-learn confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) library.\n",
    "\n",
    "The following was copied directly from the documentation to illustrate its functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 3]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [1, 0, 1, 1] # All the labels in our dataset.\n",
    "y_pred = [1, 1, 1, 1] # All of the predictions made by the model\n",
    "\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see above (in the output) is the resulting confusion matrix. A confusion matrix is a 2 by 2 matrix that holds 4 values.\n",
    "\n",
    "**Positives**\n",
    "\n",
    "1) Top left: **True positives** - values that our model correctly guessed, that were true.\n",
    "2) Top right: **False Positives** - Values that our model incorrectly guessed that were positive.\n",
    "\n",
    "**Negatives**\n",
    "\n",
    "3) Bottom Left: **False negatives** - Values that our model incorrectly guessed that were negative.\n",
    "4) Bottom right: **True negatives** - values that our model correctly guessed, that were false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
